---
sidebar_label: "游닇 Text Summarization"
sidebar_position: 11
---

# 游닇 Text Summarization

El resumen autom치tico de texto es una t칠cnica de procesamiento del lenguaje natural (NLP) que condensa uno o m치s documentos largos en un resumen m치s breve y significativo. En este proyecto, implementaremos un resumen extractivo, utilizando Python con las bibliotecas BeautifulSoup para hacer scraping web y NLTK para realizar la tokenizaci칩n y el an치lisis de frecuencia de palabras.

## 游닄 Instalaci칩n de librer칤as

Para poder realizar el resumen de texto, necesitamos instalar las siguientes librer칤as:

```bash title="Instalaci칩n de librer칤as"
pip install beautifulsoup4 lxml nltk
```

### 游볶 쯈u칠 es Beautiful Soup?

Beautiful Soup es una biblioteca de Python que facilita la extracci칩n de informaci칩n de archivos HTML y XML. Proporciona formas de navegar, buscar y modificar el 치rbol de an치lisis. Es 칰til para extraer informaci칩n de p치ginas web.

### 游 쯈u칠 es lxml?

lxml es una biblioteca de Python que permite el an치lisis y manipulaci칩n de documentos XML y HTML. Es muy r치pido y eficiente, y proporciona una API muy f치cil de usar.

_Para la libreria NLTK, no es necesario hablar ya que hemos estado trabajando con ella durante el transcurso de este curso._

## 游닇 Implementaci칩n

Para realizar el resumen de texto, primero necesitamos extraer el contenido de un art칤culo de una p치gina web. Tenemos que asegurarnos que el texto que queremos analizar tenga la etiqueta `<p>` para poder extraerlo correctamente. Ya que si no esta cargado de esta manera ser치 m치s complicado extraer el texto o no podremos extraerlo.

### 1. Extracci칩n de contenido web 游돚

El primer paso es descargar el contenido HTML de la p치gina web que deseamos analizar. Utilizaremos urllib para obtener el HTML de la p치gina y BeautifulSoup para procesarlo.

```python title="Extracci칩n de contenido web"

import bs4 as bs
import urllib.request

# Descargar el contenido HTML de la p치gina
source = urllib.request.urlopen("{Nuestra pagina web con el texto que queremos sumarizar}").read()

# Analizar el HTML con BeautifulSoup
soup = bs.BeautifulSoup(source, "lxml")

# Extraer el idioma de la p치gina
html = soup.find("html")["lang"]
lang = "spanish" if "es" in html else "english"
```

En este c칩digo, se obtiene el HTML de la p치gina y se determina el idioma de la p치gina utilizando el atributo lang del elemento `<html>`

### 2. Limpieza del texto 游빛

El siguiente paso es extraer los p치rrafos del HTML y limpiar el texto eliminando referencias, espacios adicionales y caracteres no alfab칠ticos.

```python title="Limpieza del texto"
# Extraer los p치rrafos del HTML
text = ""
for paragraph in soup.find_all("p"):
    text += paragraph.text + " "

# Limpiar el texto
import re

text = re.sub(r"\[[0-9]*\]", " ", text) # Eliminar referencias
text = re.sub(r"\s+", " ", text) # Eliminar espacios adicionales
text = re.sub(r"[^a-zA-Z]", " ", text) # Eliminar caracteres no alfab칠ticos
text = re.sub(r"\s+", " ", text) # Eliminar espacios adicionales
```

En este c칩digo, se extraen los p치rrafos del HTML y se limpia el texto utilizando expresiones regulares para eliminar referencias, espacios adicionales y caracteres no alfab칠ticos.

### 3. Tokenizaci칩n y eliminaci칩n de stopwords 游띔

Una vez que el texto est치 limpio, utilizamos NLTK para tokenizar las palabras y eliminar las palabras vac칤as (stopwords) que no aportan informaci칩n importante. Este proceso ya lo hemos visto en secciones anteriores.

```python title="Tokenizaci칩n y eliminaci칩n de stopwords"
from nltk.corpus import stopwords

# Eliminar stopwords
stop_words = set(stopwords.words(lang))
words = nltk.word_tokenize(text)
sentences = nltk.sent_tokenize(text)
noStopWords = [word for word in words if word.lower() not in stop_words]
```

### 4. An치lisis de frecuencia de palabras 游늵

El siguiente paso es calcular la frecuencia de cada palabra en el texto. Calculamos la frecuencia de cada palabra en el texto utilizando nltk.FreqDist para determinar las palabras m치s comunes, que ser치n clave para puntuar las oraciones.

```python title="An치lisis de frecuencia de palabras"
frequencies = nltk.FreqDist(noStopWords)
frequencies.most_common(10) # Mostrar las 10 palabras m치s comunes
```

Ahora que tenemos las palabras m치s comunes, podemos obtener la frequencia maxima que la usaremos para calcular la puntuaci칩n de las oraciones. Ademas podemos seleccionar el n칰mero de oraciones que queremos en nuestro resumen.

```python title="Frecuencia m치xima y n칰mero de oraciones"
maxFrequency = frequencies.most_common(1)[0][1]
numberOfSentences = 5
```

### 5. Puntuaci칩n de las oraciones 游

El 칰ltimo paso consiste en calcular la relevancia de cada oraci칩n en funci칩n de las palabras que contiene. Utilizamos la frecuencia de las palabras como indicador de la importancia de las mismas. Las oraciones que contienen palabras con mayor frecuencia son las m치s relevantes para el resumen.

Primero, inicializamos un diccionario para almacenar las puntuaciones de cada oraci칩n.

```python title="Inicializar diccionario para almacenar las puntuaciones de las oraciones"
# Inicializar diccionario para almacenar las puntuaciones de las oraciones
sentenceScores = defaultdict(int)
```

A continuaci칩n, recorremos cada oraci칩n del texto original y analizamos cada palabra en la oraci칩n. Para cada palabra cuya frecuencia sea significativa, sumamos su frecuencia normalizada (frecuencia de la palabra dividida por la frecuencia m치xima) a la puntuaci칩n de la oraci칩n.

```python title="Calcular la puntuaci칩n de las oraciones"
for sentence in sentences:
    sentenceWords = nltk.word_tokenize(sentence.lower())
    if len(sentenceWords) < 25:  # Filtrar oraciones muy cortas
        continue
    for word in sentenceWords:
        if word in frequencies:
            sentenceScores[sentence] += frequencies[word] / maxFrequency
```

En este c칩digo, se tokeniza cada oraci칩n y se calcula su puntuaci칩n solo si contiene m치s de 25 palabras (para evitar oraciones demasiado cortas que puedan no ser representativas).

Una vez que hemos puntuado todas las oraciones, ordenamos estas oraciones en funci칩n de su puntuaci칩n, de mayor a menor.

```python title="Ordenar las oraciones por puntuaci칩n"
# Ordenar las oraciones por puntuaci칩n
sortedSentences = sorted(sentenceScores.items(), key=lambda item: item[1], reverse=True)
```

Esto asegura que las oraciones m치s relevantes (aquellas con mayor puntuaci칩n) aparezcan primero en la lista.

Ahora, seleccionamos las N oraciones m치s relevantes, que representar치n el resumen final del texto original.

```python
# Seleccionar las N oraciones m치s relevantes
summarySentences = [sentence for sentence, score in sortedSentences[:numberOfSentences]]
```

### 6. Resultado final 游꿀

Finalmente, unimos las oraciones seleccionadas para formar el resumen final del texto original.

```python title="Resultado final"
# Unir las oraciones seleccionadas para crear el resumen
summary = " ".join(summarySentences)
print(summary)
```
