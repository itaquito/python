---
sidebar_label: '游깵 Historia del PLN'
sidebar_position: 2
---

# 游깵 Historia del Procesamiento de Lenguaje Natural

## Primeras apariciones del Procesamiento de Lenguaje Natural

Uno de los primeros problemas que intent칩 solucionar en NLP fueron los sistemas capaces de traducir textos de un idioma a otro.

Georgetown en 1964 fue uno de los primeros que utiliz칩 un peque침o vocabulario junto a un conjunto de reglas simples para traducir frases del ruso al ingl칠s.

## Teor칤a de la Gram치tica Generativa

Esta teor칤a propuesta por Noam Chomsky introdujo la idea de una gram치tica universal subyacente a todos los idiomas humanos.

Durante esta 칠poca y al querer usar reglas universales, los sistemas que procesaban lenguaje natural se vieron con el desaf칤o de capturar todas las sutilizas y excepciones del lenguaje natural.

## ELIZA y SHRDLU

Durante los a침os 60s y 70s, Joseph Weisenbaum dise침칩 ELIZA para simular una conversaci칩n. Este sistema pod칤a adaptar diferentes roles, como el de un psicoterapeuta.

![ELIZA](/img/procesamiento-de-lenguaje-natural/historia-pln/eliza.png)

Del mismo modo, Terry Winograd cre칩 SHRDLU, un sistema que permit칤a a las personas interactuar con un mundo virtual a base de simples comandos del lenguaje natural.

![SHRDLU](/img/procesamiento-de-lenguaje-natural/historia-pln/shrdlu.jpg)

Ambos de estos sistemas estaban limitados a comprender realmente el lenguaje natural de las personas. Los sistemas s칩lo utilizaban interacciones predefinidas y con contextos muy espec칤ficos.

## La ayuda de la inteligencia artificial al PLN

Como consecuencia de los avances de otras 치reas de la inteligencia artificial como la representaci칩n de conocimiento y la comprensi칩n sem치ntica, se empez칩 a plantear que las computadoras sean capaces de entender el significado del lenguaje y no s칩lo su estructura.

Sin embargo, estos sistemas eran dif칤ciles de escalar y requer칤an mucho trabajo manual para codificar el conocimiento del mundo.

## Enfoques estad칤sticos

A finales de los 70s y a principios de los 80s, se empezaron a utilizar un enfoque estad칤stico para procesar lenguaje sin la necesidad de reglas r칤gidas, d치ndole la capacidad de aprender de grandes cantidades de datos.

Fue hasta la llegada del aprendizaje autom치tico y el aumento del poder de c칩mputo, que fue cuando estos enfoques estad칤sticos tomaron vuelo y fueron plenamente aprovechados.

## El procesamiento de lenguaje natural en la actualidad

### Bidirectional Encoder Representations from Transformers (BERT)

Desarrollado por Google, es un sistema que introdujo un enfoque revolucionario en el preentrenamiento de representaciones ling칲칤sticas. Esto permiti칩 que el sistema comprendiera el significado de una palabra con base a su relaci칩n con las dem치s palabras en la oraci칩n. Esto mejor칩 en gran medida la comprensi칩n de lenguaje, ayudando a las tareas de clasificaci칩n de texto y la respuesta a preguntas.

![BERT de Google](/img/procesamiento-de-lenguaje-natural/historia-pln/bert.jpg)

### Generative pre-trained transformer (GPT) de OpenAI

Todo empez칩 con GPT-1 en 2018, que mejor칩 los niveles de la generaci칩n de texto en temas de fluidez y coherencia. GPT-2 y GPT-3 aumentaron el n칰mero de par치metros, permitiendo que el texto sea casi indistinguible al texto escrito por humanos. GPT-4 mejor칩 la comprensi칩n de textos complejos y su capacidad de interactuar en modalidades multimodales. De igual manera, este modelo puede utilizar im치genes y generar texto a partir de ellas.

Todos estos avances han permitido enriquecer y eliminar las barreras entre la interacci칩n de las personas y las computadoras. Permitiendo que las m치quinas puedan comprender y responder de manera coherente, m치s all치 del texto tradicional.

![GPT de OpenAI](/img/procesamiento-de-lenguaje-natural/historia-pln/gpt.png)
